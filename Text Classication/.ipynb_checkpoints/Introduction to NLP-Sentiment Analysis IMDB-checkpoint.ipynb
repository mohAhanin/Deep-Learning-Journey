{
 "cells": [
  {
   "cell_type": "raw",
   "id": "37f0c903",
   "metadata": {},
   "source": [
    "Scikit-Learn has three classes that handle the bulk of the work of cleaning and vectorizing text:\n",
    "\n",
    "CountVectorizer\n",
    "Creates a dictionary (vocabulary) from the corpus of words in the training text\n",
    "and generates a matrix of word counts like the one in Figure 4-1\n",
    "\n",
    "HashingVectorizer\n",
    "Uses word hashes rather than an in-memory vocabulary to produce word counts\n",
    "and is therefore more memory efficient\n",
    "\n",
    "TfidfVectorizer\n",
    "Creates a dictionary from words provided to it and generates a matrix similar to the one in Figure 4-1, but rather than containing integer word counts, the matrix contains term frequency-inverse document frequency (TFIDF) values between 0.0 and 1.0 reflecting the relative importance of individual words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e15a092b",
   "metadata": {},
   "source": [
    "All three classes are capable of converting text to lowercase, removing punctuation\n",
    "symbols, removing stop words, splitting sentences into individual words, and more.\n",
    "They also support n-grams, which are combinations of two or more consecutive\n",
    "words (you specify the number n) that should be treated as a single word."
   ]
  },
  {
   "cell_type": "raw",
   "id": "84dc5548",
   "metadata": {},
   "source": [
    "The idea is that words such as credit and score might be more meaningful if they appear next to each other in a sentence than if they appear far apart. Without n-grams, the relative proximity of words is ignored. The downside to using n-grams is that it increases memory consumption and training time. Used judiciously, however, it can make text classification models more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a533ad",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75027e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b9125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    " 'Four score and 7 years ago our fathers brought forth,',\n",
    " '... a new NATION, conceived in liberty $$$,',\n",
    " 'and dedicated to the PrOpOsItIoN that all men are created equal',\n",
    " 'One nation\\'s freedom equals #freedom for another $nation!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee11a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ago</th>\n",
       "      <th>brought</th>\n",
       "      <th>conceived</th>\n",
       "      <th>created</th>\n",
       "      <th>dedicated</th>\n",
       "      <th>equal</th>\n",
       "      <th>equals</th>\n",
       "      <th>fathers</th>\n",
       "      <th>forth</th>\n",
       "      <th>freedom</th>\n",
       "      <th>liberty</th>\n",
       "      <th>men</th>\n",
       "      <th>nation</th>\n",
       "      <th>new</th>\n",
       "      <th>proposition</th>\n",
       "      <th>score</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Line 1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line 3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line 4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ago  brought  conceived  created  dedicated  equal  equals  fathers  \\\n",
       "Line 1    1        1          0        0          0      0       0        1   \n",
       "Line 2    0        0          1        0          0      0       0        0   \n",
       "Line 3    0        0          0        1          1      1       0        0   \n",
       "Line 4    0        0          0        0          0      0       1        0   \n",
       "\n",
       "        forth  freedom  liberty  men  nation  new  proposition  score  years  \n",
       "Line 1      1        0        0    0       0    0            0      1      1  \n",
       "Line 2      0        0        1    0       1    1            0      0      0  \n",
       "Line 3      0        0        0    1       0    0            1      0      0  \n",
       "Line 4      0        2        0    0       2    0            0      0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the lines\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "word_matrix = vectorizer.fit_transform(lines)\n",
    "# Show the resulting word matrix\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "line_names = [f'Line {(i + 1):d}' for i, _ in enumerate(word_matrix)]\n",
    "df = pd.DataFrame(data=word_matrix.toarray(), index=line_names,\n",
    " columns=feature_names)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d059a",
   "metadata": {},
   "source": [
    "Observe from the output that equal and equals count as separate words, even\n",
    "though they have similar meaning. Data scientists sometimes go a step further when preparing text for machine learning by stemming or lemmatizing words. If the preceding text were stemmed, all occurrences of equals would be converted to equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd849d7",
   "metadata": {},
   "source": [
    " stemming  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b8a2b",
   "metadata": {},
   "source": [
    "lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685d961",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53379ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return re.sub(r'\\d+', '', text).lower()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', preprocessor=preprocess_text)\n",
    "word_matrix = vectorizer.fit_transform(lines)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef05d393",
   "metadata": {},
   "source": [
    "Another useful parameter to CountVectorizer is min_df, which ignores words that\n",
    "appear fewer than the specified number of times. It can be an integer specifying a minimum count (for example, ignore words that appear fewer than five times in the training text, or min_df=5), or it can be a floating-point value from 0.0 to 1.0 specifying the minimum percentage of samples in which a word must appear—for example, ignore words that appear in less than 10% of the samples (min_df=0.1). It’s great for filtering out words that probably aren’t meaningful anyway, and it reduces memory consumption and training time by decreasing the size of the vocabulary. CountVectorizer also supports a max_df parameter for eliminating words that appear too frequently.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ec29dcc",
   "metadata": {},
   "source": [
    "HashingVectorizer is useful when dealing with large datasets. Rather than store\n",
    "words in memory, it hashes each word and uses the hash as an index into an array of word counts. It can therefore do more with less memory and is very useful for reducing the size of vectorizers when serializing them so that you can restore them later The downside to HashingVectorizer is that it\n",
    "doesn’t let you work backward from vectorized text to the original text. Count\n",
    "Vectorizer does, and it provides an inverse_transform method for that purpose.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fee3ce7",
   "metadata": {},
   "source": [
    "TfidfVectorizer is frequently used to perform keyword extraction: examining a\n",
    "document or set of documents and extracting keywords that characterize their content. It assigns words numerical weights reflecting their importance, and it uses two factors to determine the weights: how often a word appears in individual documents, and how often it appears in the overall document set. Words that appear more frequently in individual documents but occur in fewer documents receive higher weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff772ad",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data/reviews.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c07f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1222e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a939d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
