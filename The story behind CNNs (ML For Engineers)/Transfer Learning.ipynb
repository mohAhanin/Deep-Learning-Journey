{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25399607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0545a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "# Load the model with pre-trained ImageNet weights\n",
    "base_model = ResNet50V2(weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529197fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50V2(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b08335e9",
   "metadata": {},
   "source": [
    "From that point, there are two ways to go about transfer learning. The first involves appending classification layers to the base model’s bottleneck layers and setting each base layer’s trainable attribute to False so that the weights, biases, and convolution kernels won’t be updated when the network is trained:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7852fde3",
   "metadata": {},
   "source": [
    "Freezing the Base Model’s Layers:\n",
    "Load the pre-trained model without the classification layers using include_top=False.\n",
    "Freeze the base model’s layers to prevent their weights from being updated during training.\n",
    "Append new classification layers to the base model.\n",
    "Compile and train the model with your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aef419b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x, y, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# Load the base model\n",
    "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model's layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, validation_split=0.2, epochs=10, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87f00044",
   "metadata": {},
   "source": [
    "The second technique is to run all the training images through the base model for feature extraction, and then run the features through a separate network containing your classification layers:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e7e2045",
   "metadata": {},
   "source": [
    "Feature Extraction:\n",
    "Run the training images through the base model to extract features.\n",
    "Use these features as input to a separate network containing your classification layers.\n",
    "Compile and train the new model with the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d516ce36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract features using the base model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m features \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a new model for classification\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract features using the base model\n",
    "features = base_model.predict(x)\n",
    "\n",
    "# Create a new model for classification\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with the extracted features\n",
    "model.fit(features, y, validation_split=0.2, epochs=10, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6ed5548",
   "metadata": {},
   "source": [
    "Which technique is better? The second is faster because the training images go\n",
    "through the bottleneck layers for feature extraction just one time rather than once per\n",
    "epoch. It’s the technique you should use in the absence of a compelling reason to do\n",
    "otherwise. The first technique is slightly slower, but it lends itself to fine-tuning, in\n",
    "which you unfreeze one or more bottleneck layers after training is complete and train\n",
    "for a few more epochs with a very low learning rate. It also facilitates data augmenta‐\n",
    "tion."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c83e0f8",
   "metadata": {},
   "source": [
    "If you use the first technique to implement transfer learning, you make predictions by\n",
    "preprocessing the images and passing them to the model’s predict method. For the\n",
    "second technique, making predictions is a two-step process. After preprocessing the\n",
    "images, you pass them to the base model’s predict method, and then you pass\n",
    "the output from that method to your model’s predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "222f91e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(x)\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m preprocess_input(x) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "x = image.img_to_array(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x) / 255\n",
    "features = base_model.predict(x)\n",
    "predictions = model.predict(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0f2e6",
   "metadata": {},
   "source": [
    "# Using Transfer Learning to Identify Arctic Wildlife\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e998c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
